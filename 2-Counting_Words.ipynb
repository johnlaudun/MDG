{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workbook 2: Counting Words\n",
    "\n",
    "Determing the frequency of words is about as fundamental as it gets. And it really is useful. When I teach \"The Most Dangerous Game\" to undergraduates, I like to throw a spreadsheet up which just lists all the words of the story, from most frequent to least, paired with their number of occurrences. I then ask students to go through the list and draw a line where the words become significant. A lot of the most common words, as we will see in a moment, do not carry much semantic weight -- they function, as linguists observe, as syntactic glue. The same can be said for words that occur infrequently, and I ask students to draw another line. Scanning through a frequency distribution, as such a list is called, is one place to start asking questions that you can then operationalize in your code.\n",
    "\n",
    "In the first workbook, we learned how to load a single file and to tokenize its contents such that we could work with it. In this workbook we are going to learn how to count words in a text. Like last time, we will see how to do this using basic Python tools and then explore how to do this with the NLTK. We will also explore some of the graphing possibilities available in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"toc\"></a>\n",
    "\n",
    "2.1. [The Built-In Way](#builtin)  \n",
    "2.2. [The PANDAS Way](#pandas)   \n",
    "2.3. [The NLTK Way](#nltkt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last time, we start with loading our text. Here we use the one-liner we discussed, and then, because our first two methods of creating a term frequency distribution, another way of describing a list of words and their counts, we are going to go ahead and turn our long string of text into a list of words, which is where almost all our work will begin so get used to doing it. Please note that, like last time, our list of words only includes words and apostrophes and all the words in lower case.\n",
    "\n",
    "Even though we aren't going to use the imported regular expression module until after we load the text, it is customary in Python, as I believe it is in other languages, to list your imports at the beginning of a block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First we load our file into a string\n",
    "mdg = open('texts/mdg.txt', 'r').read()\n",
    "\n",
    "# Then we turn that string into a list of words\n",
    "mdg_words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Built-In Way <a name=\"builtin\"></a>\n",
    "\n",
    "> Wanna get Capone? Here's how you get him. He pulls a knife, you pull a gun, he sends one of yours to the hospital, you send one of his to the morgue. That's the Chicago way, and that's how you get Capone. -- _The Untouchables_\n",
    "\n",
    "The first thing to know about Python, like a lot of languages (both natural and programming), is that there is usually more than one day to do something. While it's often the case that some of the powerful libraries, like PANDAS and NLTK, can do a lot for you without you having to write a lot of code, they are themselves built, in many instances, built using either basic Python, or some of the foundational libraries that are commonly installed. \n",
    "\n",
    "Our first way to compile a list of words and their frequencies uses one of the standard Python data types, a dictionary. A dictionary is always in the form of a **key** paired with a **value**, and you can recognize a dictionary by the use or curly braces, {}, whereas lists use square brackets, []. (There is yet another data type, a tuple that uses parentheses. More on that in a moment.)\n",
    "\n",
    "Let's load the code below, and then we can describe what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_dict = {}\n",
    "for word in mdg_words:\n",
    "    try:\n",
    "        mdg_dict[word] += 1\n",
    "    except: \n",
    "        mdg_dict[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown:\n",
    "\n",
    "* **`freq_dict = {}`** creates an empty dictionary into which we are going to place our key-value pairs, which will be the words along with the number of times they occur.\n",
    "* in the **`for`** loop that follows, we essentially either give a value of **`1`** if this the first time we are encountering the word or we add **`1`** to its count if we've seen it before. \n",
    "\n",
    "Having created a dictionary of word-count pairs, we can query it on various words to see how many times they occur. In the code below, I have \"hunter\" but you could also try hunted, night, Zaroff, Rainsford, and jungle. (Try it, but remember to lowercase Zaroff and Rainsford!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_dict[\"hunter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more work we can do here, especially if we wanted to sort out dictionary, from most frequent to least frequent, but, for the time being, let's move onto another way to count words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PANDAS Way\n",
    "\n",
    "The **`pandas`** library is useful when you find yourself wanting to work with data that looks a lot like a spreadsheet: it allows you to create rows and columns and then navigate them as part of its dataframe functionality. Less-mentioned than the dataframe is the pandas series, which still packs a lot of punch. \n",
    "\n",
    "In the code below, we take our list of words and create a pandas series with it -- imagine it like a single-columned spreadsheet. We can then work through the spreadsheet, compiling data as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mdg_series = pd.Series(mdg_words)\n",
    "\n",
    "print(mdg_series[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdg_counts = mdg_series.value_counts()\n",
    "print(mdg_counts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "figsize(12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we graph the 50 most frequent words, but feel free to put in your own values and then press the RUN button above or simply CTRL-RETURN. (One word of warning: 50 words is about as many will fit horizontally and still be readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts.iloc[0:49].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` makes it easy to save our results to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts.to_csv('mdg_word_freq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()\n",
    "for sentence in nltk.tokenize.sent_tokenize(mdg):\n",
    "    for word in nltk.tokenize.word_tokenize(sentence):\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(mdg)\n",
    "# print(sentence for sentence in sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From our experiments above we learn that approximately two thousand words are spread out over 8000 places. If averaged over the entire text, each word appears 4 times, but looking over our sorted `mdg_words` above, we can see that the word **and** appears 162 times alone. And it's not even the top 5 of most used words! \n",
    "\n",
    "In order they are:\n",
    "\n",
    "    the, 512\n",
    "    a, 258\n",
    "    he, 248\n",
    "    i, 177\n",
    "    of, 172\n",
    "    and, 164\n",
    "\n",
    "The list above is the start of a word frequency list. There are a number of ways to do this, and I will include those in separate files for your reference, but since we have started with the NLTK, I thought we would stay with it. We have a choice to make, however: we can either continue to import one tool at a time from the NLTK library, or we can just say to ourselves that we're going to be playing with a lot of the tools, so why not just bring them all into our workspace?\n",
    "\n",
    "Please note that once I've imported all of the `nltk` library, I need to tell Python that a particular tool, or function, comes from that library. Sometimes functions from two different, and large, libraries can have the same name, prepending the library name is one way that Python has of avoiding what are called \"namespace conflicts,\" which is a fancy way of saying you can't call two things by the same name. You've seen both ways of doing things now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "figsize(12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in freq_dist.most_common(10):\n",
    "    print('{}:  {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Let's graph the 50 most frequent words:\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# This shows all the words: still working on slices\n",
    "freq_dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = nltk.FreqDist(nltk.ngrams(mdg_words, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts[5].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, a word occurs four times in \"The Most Dangerous Game.\"\n",
    "\n",
    "Out of curiosity, how many words occur four times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrequency = nltk.FreqDist(mdg_tokens)\n",
    "four_times = [word for word in wordfrequency.keys() if wordfrequency[word] == 4]\n",
    "print(four_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.count(\"dangerous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
