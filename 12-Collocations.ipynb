{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my presentation at SDSS I depended upon Wordij to generate a collocation network for the text I was treating — I was demonstrating the focus on a single text that has emerged recently in corpus stylistics — along with its companion application, Visij. In the days leading up to the conference, I looked around for a Python equivalent and was, honestly, surprised not to be able to find anything. In these days following the conference, I have looked some more and I have still been unable to find anything. The only software I have seen is a part of the Lancashire group's efforts: they also produced LancsBox and I signed up for the course. (There's something I can do in the weeks ahead!)\n",
    "\n",
    "I found myself thinking about writing my own script, or scripts to do it. As I have thought about it, I have imagined the mechanics would looks little like what is described below.\n",
    "\n",
    "Just now, as I was writing this, I realized I have looked mostly for \"Python collocation network\" but I have not looked for scripts that are simply \"Python collocation\". I also realized that I could potentially either use, or base my own work, on the functionality built into the NLTK's `ngram` modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Danowski on \"Conceptualizing Semantic Networks\"\n",
    "\n",
    "The following is taken from:\n",
    "\n",
    "Danowki, J. A. 2011. Counter-Terrorism Mining for Individuals Semantically Similar to Watchlist Members. In _Counter-Terrorism and Open Source Intelligence_, 223-248. Ed. Uffe Kock Wiil. Springer. \n",
    "\n",
    "> Imagine a large group of people using the same language over time. Assume that the full text of their messages is available to you in natural language form. How would you come to some representation of what they are talking about? Your first thought may be to use traditional content analysis methods that categorize text, either manual procedures or computerized ones\n",
    "like the General Inquirer [1] or its more recent cousin LIWC [ ], or topic modeling software based on Bayesian statistical procedures [ ]. These automated procedures, while computationally sophisticated, are relatively crude at the conceptual level. They merely assign message elements or individuals to a limited number of nominal categories.\n",
    "> \n",
    "> Instead of categorizing messages, with a network perspective one can capture the relationships among words within the messages. Defining word-pair link strength as the number of times each word occurs closely in text with another, all possible word pairs have an occurrence distribution whose values range from zero on up. This ratio scale of measurement allows the use of sophisticated statistical tools from social network analysis toolkits. These enable the mapping of the structure of the word network. They identify word groups, or clusters, And quantify the structure of the network at different levels. Using these word-pair data as input to network analysis tools, you map the language landscape. On the map, instead of cities, the nodes are words. Rather than roads, there are links or edges among words.\n",
    "> \n",
    "> Travelling through the word network are fleets of social objects. These communication vehicles are the concepts, ideas, or physical things that people linguistically describe. As they link words to these vehicles in the course of their everyday informal and formal communication, this propels them\n",
    "through the network. Sometimes these movements are unplanned. At other times, groups or organizations try to manage vehicular traffic. By means of optimal messages, they try to steer vehicles in the flow of traffic away from certain words or toward them [2].\n",
    "> \n",
    "> Mathematically-based procedures have been developed to create optimal messages. These are constructed through systematic analysis of the paths connecting word nodes interest. The procedures identify the optimal association network across the aggregate social community. The underlying assumption is that stimulating associations across it is more effective as the shortest effective sequence of words based on particular constraints is selected for the message. This is because people process strings of words linearly over time, encoding and decoding them in sequences. Furthermore, the triggering of associations to words in context takes cognitive time. The most effective message, therefore, optimizes the association networks in the receivers' minds as they read or hear the message.\n",
    "> \n",
    "> In short, this paper focuses on the similarity of messages encoded by individuals, in other words, individuals’ semantic network similarity. This constitutes a new kind of network variable, in addition to cohesion based on actual communication exchange, and in addition to structural equivalence based on similarity of network position. This new network variable is semantic equivalence. Some may think this construct means entities have the same linguistic code identifier, such as the same name for a person, organization, or object. These are not semantic network characteristics but semantic attributes of some entity. They are akin to the words in a dictionary or elements of an ontology. In contrast, one can consider the networks among semantic elements encoded by persons or other social units. Our interest is in semantic encoding similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Build Notes\n",
    "\n",
    "  The \"window\" is the same thing as an ngram, but reducing the gram to simply the relationship between the base word, here the word furthest to the right, and its collocates. **Question**: it is at all interesting how close or far the words are? That is, do we want to know if a word is three steps or four steps away? **Tentative answer**: no.\n",
    "\n",
    "So, we are keeping track of word pairs, counting the number of times two words are within some distance of each other. We have:\n",
    "\n",
    "    base, collocate, frequency\n",
    "\n",
    "We can store that as a tuple or as a Pandas dataframe (which is essentially a Numpy array?).\n",
    "\n",
    "From [http://compling.hss.ntu.edu.sg/courses/hg2051/week09.html], there is the following:\n",
    "\n",
    "```python\n",
    "def collocations(words):\n",
    "    from operator import itemgetter\n",
    "\n",
    "    # Count the words and bigrams\n",
    "\n",
    "    wfd = nltk.FreqDist(words)\n",
    "    pfd = nltk.FreqDist(tuple(words[i:i+2]) for i in range(len(words)-1))\n",
    "\n",
    "    #\n",
    "    scored = [((w1,w2), score(w1, w2, wfd, pfd)) for w1, w2 in pfd]\n",
    "    ## sort according to the score\n",
    "    scored.sort(key=itemgetter(1), reverse=True)\n",
    "    return [p for (p,s) in scored]\n",
    "\n",
    "\n",
    "def score(word1, word2, wfd, pfd, power=3):\n",
    "    'return the collocation score f(w1,w2)^power/(f(w1)*f(w2))'''\n",
    "    freq1 = wfd[word1]\n",
    "    freq2 = wfd[word2]\n",
    "    freq12 = pfd[(word1, word2)]\n",
    "    return freq12 ** power / float(freq1 * freq2)\n",
    "```\n",
    "\n",
    "\n",
    "Part of a [course](http://compling.hss.ntu.edu.sg/courses/hg2051/) being offered by [Francis Bond](http://www3.ntu.edu.sg/home/fcbond/) at Nanyang Technological University. One the page for the course, he cites Jurafsky and Martin's _[Speech and Language Processing](http://www.cs.colorado.edu/~martin/slp.html)_ (2009).\n",
    "\n",
    "---\n",
    "\n",
    "**NLTK Solution?** According to the NLTK documentation on [Collocations](http://www.nltk.org/howto/collocations.html), one can set a window for bigrams. *Duh!* \n",
    "\n",
    "```python\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'),\n",
    "    window_size = 20)\n",
    "finder.apply_freq_filter(2)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "# doctest: +NORMALIZE_WHITESPACE\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiguous word collocations as directed networks get fairly close to discourse in a very compelling way. For too long, I have depended on a Java application, Wordij, both to get the data and to visualize it. What follows is my attempt to build a working codebase in Python.\n",
    "\n",
    "What the program needs to do:\n",
    "\n",
    "* accept a text or group of texts as input\n",
    "* break those strings into a series of word pairs (ideally this would include a measure of how far apart the words were)\n",
    "* compile the word pairs into a series of nodes and edges such that `word1 > word2` with a count of how many times that pair arises so that the edges can be weighted\n",
    "* visualize the network\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* window size\n",
    "* include/exclude stopwords\n",
    "* network dimensions (thresholds to be included, various network optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "* [NLTK documentation on collocations](http://www.nltk.org/howto/collocations.html)\n",
    "* [Getting the count of a collocation](https://stackoverflow.com/questions/38597503/in-nltk-get-the-number-of-occurrences-of-a-trigram)\n",
    "* This [SO thread][so] lays out the reasons for assembling all bigrams first and then filtering the less interesting ones out later. \n",
    "\n",
    "[so]: https://stackoverflow.com/questions/19145332/nltk-counting-frequency-of-bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a text:\n",
    "mdg = open('texts/mdg.txt', 'r').read()\n",
    "words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with code straight from the NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nearer', 'nearer'),\n",
       " ('general', 'zaroff'),\n",
       " ('drew', 'nearer'),\n",
       " ('nearer', 'ridge'),\n",
       " ('mr', 'rainsford')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 15\n",
    "pairs = 5\n",
    "ignore_pairs = 2\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.BigramCollocationFinder.from_words(words, window_size = window)\n",
    "finder.apply_freq_filter(ignore_pairs)\n",
    "finder.apply_word_filter(lambda w: w in ignored_words) # len(w) < 3 or w.lower()\n",
    "finder.nbest(bigram_measures.likelihood_ratio, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't entirely understand the \"measures\" being applied above. I know it's in the MLTK documentation, and in the NLP literature in general, but I would like to start my work here by getting a count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_finder = nltk.BigramCollocationFinder.from_words(words, window_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,v in my_finder.ngram_fd.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above outputs the follow:\n",
    "\n",
    "    ('off', 'there') 1\n",
    "    ('off', 'to') 2\n",
    "    ('there', 'to') 2\n",
    "    ('there', 'the') 3\n",
    "    ('to', 'the') 25\n",
    "    ('to', 'right') 2\n",
    "    ('the', 'right') 4\n",
    "\n",
    "I tried converting to a list with:\n",
    "\n",
    "    bgm_list = [ \"({}), {}\"%(k,v) for k,v in my_finder.ngram_fd.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = my_finder.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FreqDist as Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have is an NLTK Frequency Distribution. The FreqDist object in NLTK is a sub-class of the native Python's `collections.Counter`, which are dictionaries that store the elements in a list as its key and the counts of the elements as the values ([URL][]). They work like this:\n",
    "\n",
    "[URL]: https://stackoverflow.com/questions/37427673/sorting-freqdist-in-nltk-with-get-vs-get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'b': 1, 'c': 3, 'd': 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(['a','a','b','c','c','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 3), ('a', 2), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(['a','a','b','c','c','c','d'])\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1), ('b', 1), ('a', 2), ('c', 3)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(c.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b', 'c', 'd'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 1), ('c', 3), ('d', 1)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data out of the `FreqDist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrams.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs the following:\n",
    "\n",
    "```\n",
    "[(('the', 'general'), 74),\n",
    " (('of', 'the'), 51),\n",
    " (('the', 'of'), 50),\n",
    " (('in', 'the'), 37),\n",
    " (('the', 'and'), 28),\n",
    " (('it', 'was'), 27),\n",
    " (('to', 'the'), 25),\n",
    " (('a', 'of'), 25),\n",
    " (('he', 'was'), 24),\n",
    " (('the', 'was'), 22),\n",
    " (('he', 'the'), 22),\n",
    " (('was', 'the'), 22),\n",
    " (('rainsford', 'the'), 21),\n",
    " ...]\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a list of tuples, each consisting of a tuple and an integer.\n",
    "\n",
    "Now we can return to the code above:\n",
    "\n",
    "```python\n",
    "for k,v in finder.ngram_fd.items():\n",
    "    print(k,v)\n",
    "```\n",
    "\n",
    "with the understanding that the pair of words, the bigram, is the key and the count is the value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is adapted from a question about [NLTK trigrams](https://stackoverflow.com/questions/38597503/in-nltk-get-the-number-of-occurrences-of-a-trigram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgm_counts = sorted(my_finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12234"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bgm_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'general'), 74), (('of', 'the'), 51), (('the', 'of'), 50), (('in', 'the'), 37), (('the', 'and'), 28)]\n"
     ]
    }
   ],
   "source": [
    "print(bgm_counts[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Network\n",
    "\n",
    "See: \n",
    "* [Exploring and Analyzing Network Data with Python | Programming Historian](https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python)\n",
    "* And it looks like loading a graph in NetworkX is easier than I imagined: [Tutorial — NetworkX 2.4 documentation](https://networkx.github.io/documentation/stable/tutorial.html#adding-attributes-to-graphs-nodes-and-edges)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}