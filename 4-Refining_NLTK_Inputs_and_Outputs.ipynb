{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Refining NLTK Inputs & Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Table of Contents <a name=\"toc\"></a>\n",
    "\n",
    "**[Loading a File & Understanding What It Is](#file)**  \n",
    "**[Tokenizing](#tokenizing)**  \n",
    "[A Quick Note about Normalization](#norm)  \n",
    "[Using regex to Tokenize](#REtoke)   \n",
    "[Using NLTK Tokenizers](#nltktoken)  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the first two notebooks we learned how to load a file, create a word list out of a text string, and then to count words and visualize those counts. In this notebook we will use the `NLTK` to explore how various words occur or are used within a text. Once you have seen how these commands work, feel free to restart the notebook and upload your own text. In the next notebook, we will look at how to load more than one text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Text, Create Word List, Create NLTK `Text`\n",
    "\n",
    "The first thing we are going to do is load the libraries we know we are going to use, load our text file, and create our word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "file = open('texts/mdg.txt', 'r').read()\n",
    "words = re.sub(\"[^a-zA-Z']\",\" \", file).lower().split()\n",
    "text = nltk.Text(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "In the previous notebook we explored how to use the NLTK's `concordance()` functionality and we discovered some limitations to its default settings. In this notebook we will explore how we can write some fairly basic blocks of code to get the kinds of outputs we want.\n",
    "\n",
    "Recall that the basic command looks like this:\n",
    "\n",
    "```python\n",
    "my_text.concordance(\"word\")\n",
    "```\n",
    "\n",
    "You can type the line just like that, and Jupyter notebook will deliver results, but please note that Jupyter notebook is simply displaying what Python is delivering to standard output, often known as `stdout`. If ever you enter a command and you don't get stdout, just add print functionality. \n",
    "\n",
    "One of the limitations of of a number of NLTK's functions is that they default to 25 lines of output. `concordance` will serve as our example here. A quick check of the [documentation][] reveals that the default is `concordance(word, width=79, lines=25)`. (Yes, the NLTK documentation is online: you can read all its lines of codes for yourself -- try that with Microsoft Word!)\n",
    "\n",
    "[documentation]: http://www.nltk.org/_modules/nltk/text.html#Text.concordance\n",
    "\n",
    "In order to find words that will exceed that limit but not by so much that there is a lot to scroll through, I wrote the following small bit of code to find those words for me. A quick description of what this code does:\n",
    "\n",
    "* **`for word in set(words):`** tells Python to go through all the words in the text, but only once for each word, which we can do by reducing the text to the set of words used.\n",
    "* **`if 25 < text.count(word) < 30:`** tells Python only to select the words whose count is greater than 25, exceeding the default, but less than 30 (but this could have been a larger number).\n",
    "* **`print(word, text.count(word)`** prints the word and the number of times it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word in set(words):\n",
    "    if 25 < text.count(word) < 30:\n",
    "        print(word, text.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hunting\" will work. Let's see what happens when we add parameters for \"hunting\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance(\"hunting\", width=79, lines=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! Let's add two revisions:\n",
    "\n",
    "* A little experimentation reveals that we don't need to specify `width` if don't wish to change it. \n",
    "* We don't need to feed the number of lines by hand: we can let `count()` do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"hunting\"\n",
    "text.concordance(word, lines=text.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook on the NLTK we tried a list of words that just so happened to be related:\n",
    "\n",
    "```python\n",
    "word_list = ['hunter', 'hunted', 'hunt']\n",
    "for word in word_list:\n",
    "    print(mdg_text.concordance(word))\n",
    "```\n",
    "\n",
    "And we wondered if we couldn't ask Python to stem, or lemmatize, for us. It turns out you can. The following example of code uses the Porter stemmer, but the Lancaster stemmer is also available, as is a lemmatizer that uses WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for word in set(words):\n",
    "    if porter.stem(word) == \"hunt\":\n",
    "        print(text.concordance(word, lines=text.count(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure why \"hunter\" doesn't make the list. I'll keep looking.\n",
    "\n",
    "Clai asked about sorting on n-1 or n+1. I do not yet see an easy way to do this. I'll keep looking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
