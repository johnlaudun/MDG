{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Files to Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"toc\"></a>\n",
    "\n",
    "**[Loading a File & Understanding What It Is](#file)**  \n",
    "**[Tokenizing](#tokenizing)**  \n",
    "[A Quick Note about Normalization](#norm)  \n",
    "[Using regex to Tokenize](#REtoke)   \n",
    "[Using NLTK Tokenizers](#nltktoken)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first line of code run _here_ is something internal to Jupyter Notebooks that allows us to place any graphical output into the page itself and not in a separate window or file. (We can still save output to a file, if we want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "figsize(12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a File & Understanding What It Is <a name=\"file\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After that, it's time to get our text and start examining it. So the first thing we need to do is load the text file. In the case of reading one file, as we are doing here, we first tell Python to open the file in **read** mode -- that's all the `r` is doing inside the parenthesis, preventing any possibility of of us writing to it. After that we literally read the file into a variable. \n",
    "\n",
    "This can be done in two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we open the file:\n",
    "opened_file = open('texts/mdg.txt', 'r')\n",
    "\n",
    "# Then we read the file:\n",
    "mdg = opened_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to work with the first object we created, `opened_file`, let's say by printing it, we are going to get an error. Python recognizes that it is probably a text file, but it hasn't been told specifically to take the stream of bytes, as Python understand them, and convert them into a string, which is how computers handle things humans call \"texts.\"\n",
    "\n",
    "In the code cells above and below you will also notice the presence of lines that are simply descriptions of what the code does and not themselves code: that is, they do not do anything but communicate to humans, yourself or others, what is happening in the code. These are called comments, and it's good to get in the habit of including them. Comments start with a hash mark, `#`, and are typically hand-wrapped if more than one line, with each line beginning with a hash mark. In Jupyter Notebook, you can simply select the lines you want to make comments and then `CMD + /` on your keyboard. This is a toggle-able action, so you can uncomment code this way too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when we try to print the opened file?\n",
    "print(opened_file[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The work of the two lines above can also be achieved in one line.\n",
    "# See if your growing Pythonista abilities can't tell how this is done!\n",
    "mdg = open('texts/mdg.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, you've just created your first \"object\" in Python, and it's a text! Or, rather, it's a string, one of the kinds of objects you can work with. If you ever wonder what kind of object you have, you can ask it its `type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask it other kinds of things: how big, or long it is -- `len()` -- as well as printing it to see what it looks like. Why don't you do that now? Replace `type` above first with `len` and then with `print` and then hit enter to see what happens. \n",
    "\n",
    "When you run **`print(mdg)`**, you see all of the text in what appears to be \"human readable\" form. One of the problems you now face is understanding that when you hit `print` and see the text, the computer just sees a `string` of characters -- remember what `type` told you? Python doesn't natively understand human languages: they are nothing more than a series of things, characters made up of letters, numbers, punctuation marks, and spaces.\n",
    "\n",
    "When you asked Python to tell you the length of the object, it just counted all those things and told you the total. Our version of \"The Most Dangerous Game\" is 44,000+ characters long. But characters isn't a very useful way to measure texts, is it? Letters are not meaningful. Words are. That's how we think of texts, isn't it? In order to count the words, we have to tell Python how to break the string into words.\n",
    "\n",
    "We need to convert our string into a list of words. To do that, we need to figure out how to tell the computer to find words among the sequence of characters. The term for words, and all the other bits that make up written language (like punctuation) as they occur in discourse is **tokens**, and what we need to do is **tokenize**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing <a name=\"tokenizing\"></a>\n",
    "\n",
    "If only **tokenization** was straightforward. While your first essays into text analytics will probably rely upon relatively well-known methods, you will probably find yourself regularly re-visiting what you use and fine-tuning to fit the needs of your project. (That kind of iterative understanding and refinement based on the work at hand is part of what makes work in the digital humanities so rewarding.) In other words, it's important to remember that, first, tokenization refers \"the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens\" ([Wikipedia][]) and that \"meaningful\" is in the eye of the beholder.\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\n",
    "\n",
    "\n",
    "One of the first places to start tokenizing is with the `split()` method available to use on strings, turning them into lists. If you supply no value inside the parentheses, the default  for `split()` is to split a string into a list wherever there is a white space. \n",
    "\n",
    "In the cell below, we first create a string object called `text` -- in this case, the first paragraph from _Heart of Darkness_ -- and then we `split()` it and print the list to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Nellie, a cruising yawl, swung to her anchor without a flutter of\n",
    "the sails, and was at rest. The flood had made, the wind was nearly\n",
    "calm, and being bound down the river, the only thing for it was to come\n",
    "to and wait for the turn of the tide.\n",
    "\"\"\"\n",
    "\n",
    "split_text = text.split()\n",
    "\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>>>**: Why not copy and paste a block of text from your own work to see what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually specify other values on which to split a string, which can be useful in some cases. (Please note that `split()` and `split(' ')` are the same thing, but do note that there is a space between the two quotation marks.)\n",
    "\n",
    "In the next cell, we split the paragraph by comma -- the `\\n`s you see are newline characters normally hidden from human eyes, but always there. They care considered part of white space by Python, which is why they are not part of the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comma_text = text.split(\",\")\n",
    "print(comma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Python's built-in string method `split()` has plenty of power and may very well do everything you need for quick surveying of results. For more nuanced results, you may find yourself wanting to build a custom setup with regex or to use one of the tokenizers included with the NLTK. Both of those are discussed next.\n",
    "\n",
    "[For more on tokenization in Python.](http://jeffreyfossett.com/2014/04/25/tokenizing-raw-text-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Note about Normalization <a name=\"norm\"></a>\n",
    "\n",
    "In the first sentence from the opening of _Heart of Darkness_ there are two instances of the definite article *the*:\n",
    "\n",
    "> The Nellie, a cruising yawl, swung to her anchor without a flutter of the sails, and was at rest.\n",
    "\n",
    "If we break the sentence into tokens as above and then ask Python to compose a set of words based on the sentence we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever you have extra long text in a code example that breaks across lines,\n",
    "# you enclose it within three quotation marks to let Python know that. \n",
    "# (This is one reason to load text from a file.)\n",
    "\n",
    "sentence = \"\"\"The Nellie, a cruising yawl, swung to her anchor without a flutter of\n",
    "the sails, and was at rest.\"\"\"\n",
    "\n",
    "split = sentence.split()\n",
    "\n",
    "print(\"This set has {} items:\".format(len(set(split))), set(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if we ask Python is *The* and *the* are the same, it tells us no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'The' == 'the'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way most scholars and scientists avoid this particular semantic difficulty is to make all the text in a string lowercase with `lower()`. This too is a string method, and so you can append it just as you do `split()`. (FTR, there is also `upper()` that makes everything uppercase.)\n",
    "\n",
    "Watch what happens when normalize our text to lowercase and ask Python to count the set of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"The Nellie, a cruising yawl, swung to her anchor without a flutter of\n",
    "the sails, and was at rest.\"\"\"\n",
    "\n",
    "split = sentence.lower().split()\n",
    "\n",
    "print(\"This set has {} items:\".format(len(set(split))), set(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two *the*s are now considered to be the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regex to Tokenize <a name=\"REtoke\"></a>\n",
    "\n",
    "As you are already beginning to sense, there are a lot of ways to approach breaking a string of characters into a string of words. The first method presented above is the most basic and should be used when first starting out, especially when you pair lowercasing with it -- `.lower().split()` are your new best friends. But at some point you may find that you want a little bit more flexibility in how you handle your text. \n",
    "\n",
    "This next approach still depends upon `.lower().split()`, but before we pass our text to it, we do a bit of cleanup. It's one I have used quite often when making first passes through texts. After a while, you'll find that some bits of regex become part of your toolset, and reveal, as this one does mine, your own interests and concerns. In this case, the regex reveals a particular obsession of mine: I like to keep contractions together. (This reveals my own non-standard training: corpus linguists just take for granted that **`can't`** becomes **`ca`** and **`n't`**, or something else equally weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the regular expression library\n",
    "import re\n",
    "\n",
    "mdg_words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick walkthrough might help a little:\n",
    "\n",
    "* **`import re`** tells the script to import the regular expression module which comes bundled with every Python installation but doesn't get loaded into our workspace unless we tell it to do so.\n",
    "* **`mdg_words`** is the object we are creating: everything to the right is the process by which we create the object.\n",
    "* We aren't going to discuss the regular expression substitution, **`re.sub()`** that gets done here, except to note that you should read the stuff inside the parentheses like this: `(find this pattern, substitute this, in this text)` -- in this case I am telling it to find things that are **not** (`[^ ]` is called a *negated set*) letters (big and small) or apostrophes and replace them with spaces. \n",
    "* **`.lower()`** is a method you can apply to strings that makes everything lower case -- otherwise \"The\" and \"the\" are two different keys. \n",
    "* **`.split()`** is the string method discussed above and we're using its default setting of splitting on white spaces, of which we have plenty, since we have replaced everything except for letters and apostrophes with white space.\n",
    "\n",
    "The **`split()`** method turns our string into a **list**. In this case, a list of words that are in the same order as they are in the original text. (The computer has no reason to disturb this order.)\n",
    "\n",
    "If we ask how long the list is, we should come back with a reasonably close count of the words -- don't forget we kept apostrophes, and while most are buried inside contractions, there may be some loose apostrophes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words in text: {}.'.format(len(mdg_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will discover, or have discovered, a list is a different kind of object than a string, and it has some useful properties. One of the things we can do is **slice** a list. In the command below we are saying we want the first 50 words in the list of words: start at the 0th element and go up to the 50th element. Note how the apostrophe, here as a single quotation mark inside Whitney's response about \"'ship trap island'\", is part of our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(mdg_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK Tokenizers <a name=\"nltktoken\"></a>\n",
    "\n",
    "Whether you develop your own tokenizer somewhere along the way or not is entirely up to you and the nature of the projects which you undertake. You may never find yourself doing so. There are certainly a lot of already available options, a number of which are packaged with the Natural Language Toolkit, more often called by its acronym \"NLTK\", which, by the way, is the same way we call it in Python: \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "\n",
    "As you saw above, the way we tell Python we want some additional functionality is to tell it to **import** a particular library. Previously, we imported the regular expression library, which is called **`re`**. \n",
    "\n",
    "If we were to run the line above, we would probably wait a few seconds as the NLTK library loaded -- it's quite large. Because it is a large library, and we really don't need all its functionality, most people don't load all of it -- and isn't it cool that we can load only the parts we need! This is handy as you begin to work with larger scripts and larger data sets: keeping your workspace as tidy, and as small, becomes a necessity. And, in some cases, it's actually easier to use certain tools singly and with a particular name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this instance, we are going to tell Python that we only want \n",
    "# one particular tool from the larger toolkit:\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's `WhitespaceTokenizer` is about as basic as it comes -- in fact, in the NLTK documentation, they actually note you might as well use split. It is, so far as I can tell, its output is the same as `.lower().split()` while also keeping contractions together. However, do note that punctuation is kept with words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_tokens = WhitespaceTokenizer().tokenize(mdg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please note that we now have two versions of \"The Most Dangerous Game\" rendered as a list in Python: **`mdg_words`** and **`mdg_tokens`**. We created the former using regex and the latter with the NLTK WhitespaceTokenizer. We could have re-used the former name, and that would have, in effect, replaced the older list with the newer one. Please keep this in mind as you work, and also remember to use sensible names for the objects you create. (The more self-explanatory things are, the better as your code grows.)\n",
    "\n",
    "Now, let's take a look at what kind of object this is, how big it is, and then let's print the first 50 items -- I use 50 as a convenient, quick look, but you can adjust the number down to 25 or up to 100 or some other number that you find more useful. I often change the numbers several times, contracting and expanding as I feel the need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdg_tokens[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would happen if we were to read the text differently, if we were to read all the words, but this time with the words in alphabetical order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(mdg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's this? That's a lot of *a*s and *about*s. What happens if we look at just the words without repetition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set(mdg_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That looks like a lot of words. If we ask how many by counting how long the set of words is, we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how I enclose the text I want to print in single quotes\n",
    "# so that I can use double quotes in the text itself:\n",
    "print('There are {} words in \"The Most Dangerous Game.\"'.format(len(set(mdg_tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()\n",
    "for sentence in nltk.tokenize.sent_tokenize(mdg):\n",
    "    for word in nltk.tokenize.word_tokenize(sentence):\n",
    "        fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(mdg)\n",
    "# print(sentence for sentence in sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From our experiments above we learn that approximately two thousand words are spread out over 8000 places. If averaged over the entire text, each word appears 4 times, but looking over our sorted `mdg_words` above, we can see that the word **and** appears 162 times alone. And it's not even the top 5 of most used words! \n",
    "\n",
    "In order they are:\n",
    "\n",
    "    the, 512\n",
    "    a, 258\n",
    "    he, 248\n",
    "    i, 177\n",
    "    of, 172\n",
    "    and, 164\n",
    "\n",
    "The list above is the start of a word frequency list. There are a number of ways to do this, and I will include those in separate files for your reference, but since we have started with the NLTK, I thought we would stay with it. We have a choice to make, however: we can either continue to import one tool at a time from the NLTK library, or we can just say to ourselves that we're going to be playing with a lot of the tools, so why not just bring them all into our workspace?\n",
    "\n",
    "Please note that once I've imported all of the `nltk` library, I need to tell Python that a particular tool, or function, comes from that library. Sometimes functions from two different, and large, libraries can have the same name, prepending the library name is one way that Python has of avoiding what are called \"namespace conflicts,\" which is a fancy way of saying you can't call two things by the same name. You've seen both ways of doing things now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in freq_dist.most_common(10):\n",
    "    print('{}:  {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in freq_dist:\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdg_dict = {key:value for word, frequency in freq_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Let's graph the 50 most frequent words:\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# This shows all the words: still working on slices\n",
    "freq_dist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Save these results to a CSV file (makes it easier for the Excel-impaired)\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "mdg_counts.to_csv('../data/mdg_word_freq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = nltk.FreqDist(nltk.ngrams(mdg_words, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts[5].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('ggplot')\n",
    "ax = df[['Word','Frequency']].plot(kind='bar', \n",
    "                                   title =\"Frequency of Words in MDG\",\n",
    "                                   figsize=(20,10),\n",
    "                                   legend=True)\n",
    "ax.set_xlabel(\"Word\")\n",
    "ax.set_ylabel(\"Occurrences\")\n",
    "ax.set_xticklabels(list(df['Word'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myword = mdg_words.concordance(\"dangerous\")\n",
    "print(myword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar(\"love\")\n",
    "text.common_contexts([\"husband\", \"wife\"])\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Diversity of MDG:\n",
    "len(mdg2_word_list) / len(set(mdg2_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mdg_tokens) / len(set(mdg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, a word occurs four times in \"The Most Dangerous Game.\"\n",
    "\n",
    "Out of curiosity, how many words occur four times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrequency = nltk.FreqDist(mdg_tokens)\n",
    "four_times = [word for word in wordfrequency.keys() if wordfrequency[word] == 4]\n",
    "print(four_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.count(\"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.concordance(\"dangerous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does \"dangerous\" occur within the larger text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.dispersion_plot([\"dangerous\", \"danger\", \"game\", \"fear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrequency.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
