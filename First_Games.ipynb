{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Most Dangerous Game of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first line of code run here is something internal to Jupyter Notebooks that allows us to place any graphical output into the page itself and not in a separate window or file. (We can still save output to a file, if we want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "figsize(12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After that, it's time to get our text and start examining it. So the first thing we need to do is load the text file. In the case of reading one file, as we are doing here, we first tell Python to open the file in **read** mode -- that's all the `r` is doing inside the parenthesis, preventing any possibility of of us writing to it. After that we literally read the file into a variable. \n",
    "\n",
    "This can be done in two lines:\n",
    "\n",
    "```python\n",
    "opened_file = open('/texts/mdg.txt', 'r')\n",
    "mdg = opened_file.read()\n",
    "```\n",
    "\n",
    "Or it can be done in one line as it is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mdg = open('texts/mdg.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, you've just created your first \"object\" in Python, and it's a text! Or, rather, it's a string, one of the kinds of objects you can work with. If you ever wonder what kind of object you have, you can ask it its `type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask it other kinds of things: how big, or long it is -- `len()` -- as well as printing it to see what it looks like. Why don't you do that now? Replace `type` above first with `len` and then with `print` and then hit enter to see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see all of the text, in a \"human readable\" form. \n",
    "One of the problems you now face is understanding that when you hit `print` and see the text, the computer just sees a `string` of characters -- remember what `type` told you? Python doesn't natively understand human languages: they are nothing more than a series of things, characters made up of letters, numbers, punctuation marks, and spaces.\n",
    "\n",
    "When you asked Python to tell you the length of the object, it just counted all those things and told you the total. Our version of \"The Most Dangerous Game\" is 44078 characters long. But characters isn't a very useful way to measure texts, is it? Letters are not meaningful. Words are. That's how we think of texts, isn't it? In order to count the words, we have to tell Python how to break the string into words.\n",
    "\n",
    "We need to convert our string in a list of words, which, it turns out, is still human readable. To do that, we need to figure out how to tell the computer to find words among the sequence of characters. The term for words as they are found in discourse is **tokens**, and what we need to do is **tokenize**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", mdg).lower().split()\n",
    "print('Words in text: {}.'.format(len(mdg_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, I have come to develop my own tokeniser, which I have inserted above just so we can talk about it, but for our purposes today, I am going to turn to the Natural Language Toolkit, more often called by its acronym, which, by the way, is the same way we call it in Python: \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "\n",
    "But the NLTK is a rather large library, or module -- we'll discuss this! -- and as you begin to work with larger programs and larger collections of texts, you want to keep your space as tidy as possible and only get out the tools you need. (And, in some cases, tools prefer to be pulled out singly which also makes their use easier.) \n",
    "\n",
    "In this instance, we are going to tell Python that we only want one particular tool from the larger toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_tokens = WhitespaceTokenizer().tokenize(mdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's take a look at what kind of object this is, how big it is, and then let's print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Off', 'there', 'to', 'the', 'right', '--', 'somewhere', '--', 'is', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(mdg_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would happen if we were to read the text differently, if we were to read all the words, but this time with the words in alphabetical order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(mdg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's this? That's a lot of *a*s and *about*s. What happens if we look at just the words without repetition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set(mdg_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That looks like a lot of words. If we ask how many by counting how long the set of words is, we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(mdg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Almost two thousand words are spread out over 8000 places. If averaged over the entire text, each word appears 4 times, but looking over our sorted `mdg_words` above, we can see that the word **and** appears 162 times alone. And it's not even the top 5 of most used words! \n",
    "\n",
    "In order they are:\n",
    "\n",
    "    the, 512\n",
    "    a, 258\n",
    "    he, 248\n",
    "    i, 177\n",
    "    of, 172\n",
    "    and, 164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To be clear, these numbers are drawn from a complete list of the words in \"The Most Dangerous Game\" that was compiled by first creating a dataframe with each word on its own, indexed, row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mdg_series = pd.Series(mdg_words)\n",
    "\n",
    "print(mdg_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can take that sequence of words and count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mdg_counts = mdg_series.value_counts()\n",
    "print(mdg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Let's graph the 50 most frequent words:\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "mdg_counts.iloc[50:99].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Save these results to a CSV file (makes it easier for the Excel-impaired)\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "mdg_counts.to_csv('../data/mdg_word_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('ggplot')\n",
    "ax = df[['Word','Frequency']].plot(kind='bar', \n",
    "                                   title =\"Frequency of Words in MDG\",\n",
    "                                   figsize=(20,10),\n",
    "                                   legend=True)\n",
    "ax.set_xlabel(\"Word\")\n",
    "ax.set_ylabel(\"Occurrences\")\n",
    "ax.set_xticklabels(list(df['Word'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "myword = mdg.concordance(\"dangerous\")\n",
    "print(myword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar(\"love\")\n",
    "text.common_contexts([\"husband\", \"wife\"])\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mdgtokens = nltk.word_tokenize(mdg)\n",
    "len(mdgtokens)\n",
    "\n",
    "import nltk, re\n",
    "\n",
    "mdg_raw = open(\"./mdg.txt\").read()\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", mdg_raw)\n",
    "mdg_case = mdg_words.lower()\n",
    "\n",
    "# print(mdg_case)\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "mdg_word_list = mdg_words.split()\n",
    "print(mdg_word_list)\n",
    "\n",
    "sorted(set(mdg_word_list))\n",
    "\n",
    "len(sorted(set(mdg_word_list)))\n",
    "\n",
    "# Lexical Diversity of MDG:\n",
    "len(mdg2_word_list) / len(set(mdg2_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mdg_tokens) / len(set(mdg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, a word occurs four times in \"The Most Dangerous Game.\"\n",
    "\n",
    "Out of curiosity, how many words occur four times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrequency = nltk.FreqDist(mdg_tokens)\n",
    "four_times = [word for word in wordfrequency.keys() if wordfrequency[word] == 4]\n",
    "print(four_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.count(\"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.concordance(\"dangerous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does \"dangerous\" occur within the larger text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.dispersion_plot([\"dangerous\", \"danger\", \"game\", \"fear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrequency.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
